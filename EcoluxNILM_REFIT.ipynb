{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31352643-9820-4cd7-929a-ce278386e45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the same thing to convert any dataset\n",
    "source_dir = 'C:\\\\Users\\\\nicho\\\\OneDrive - The University of Western Ontario\\\\Ecolux\\\\Databases\\\\REFIT\\\\Original CSV'\n",
    "out_dir = 'C:\\\\Users\\\\nicho\\\\OneDrive - The University of Western Ontario\\\\Ecolux\\\\Databases\\\\REFIT\\\\refit.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a80b78-46cd-4253-9c91-c543d7dfe399",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Only change this to code if you want to convert everything again\n",
    "from nilmtk.dataset_converters import convert_refit\n",
    "\n",
    "convert_refit(source_dir, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc812c6b-cf14-41ec-85d5-d8b41c5c6165",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ul><li><strong>name</strong>: REFIT</li><li><strong>long_name</strong>: REFIT Electrical Load Measurements dataset</li><li><strong>creators</strong>: <ul><li>Murray, David</li><li>Stankovic, Lina</li></ul></li><li><strong>publication_date</strong>: 2015</li><li><strong>institution</strong>: University of Strathclyde</li><li><strong>contact</strong>: david.murray.2013@uni.strath.ac.uk</li><li><strong>description</strong>: Up to 2 years of power data for 20 UK homes.</li><li><strong>subject</strong>: Disaggregated power demand from domestic buildings.</li><li><strong>number_of_buildings</strong>: 20</li><li><strong>timezone</strong>: Europe/London</li><li><strong>geo_location</strong>: <ul><li><strong>locality</strong>: Loughborough</li><li><strong>country</strong>: UK</li><li><strong>latitude</strong>: 52.769655</li><li><strong>longitude</strong>: -1.225061</li></ul></li><li><strong>related_documents</strong>: <ul><li><a href=\"http://iet.jrc.ec.europa.eu/energyefficiency/sites/energyefficiency/files/events/EEDAL15/S25_Smart_Meters/eedal15_submission_124.pdf\">http://iet.jrc.ec.europa.eu/energyefficiency/sites/energyefficiency/files/events/EEDAL15/S25_Smart_Meters/eedal15_submission_124.pdf</a></li><li>David Murray, Jing Liao, Lina Stankovic, Vladimir Stankovic, Richard Hauxwell-Baldwin, Charlie Wilson, Michael Coleman, Tom Kane, Steven Firth. A data management platform for personalised real-time energy feedback. In Procededings of the 8th International Conference on Energy Efficiency in Domestic Appliances and Lighting, 2015.\n",
       "</li></ul></li><li><strong>schema</strong>: <a href=\"https://github.com/nilmtk/nilm_metadata/tree/v0.2\">https://github.com/nilmtk/nilm_metadata/tree/v0.2</a></li><li><strong>meter_devices</strong>: <ul><li><strong>IAM</strong>: <ul><li><strong>model</strong>: unknown</li><li><strong>manufacturer</strong>: unknown</li><li><strong>manufacturer_url</strong>: unknown</li><li><strong>description</strong>: individual appliance monitor</li><li><strong>sample_period</strong>: 7</li><li><strong>max_sample_period</strong>: 120</li><li><strong>measurements</strong>: <ul><li>{'physical_quantity': 'power', 'type': 'active', 'upper_limit': 5000, 'lower_limit': 0}</li></ul></li><li><strong>wireless</strong>: True</li></ul></li><li><strong>Clamp</strong>: <ul><li><strong>model</strong>: unknown</li><li><strong>manufacturer</strong>: unknown</li><li><strong>manufacturer_url</strong>: unknown</li><li><strong>description</strong>: current clamp</li><li><strong>sample_period</strong>: 7</li><li><strong>max_sample_period</strong>: 120</li><li><strong>measurements</strong>: <ul><li>{'physical_quantity': 'power', 'type': 'active', 'upper_limit': 5000, 'lower_limit': 0}</li></ul></li><li><strong>wireless</strong>: True</li></ul></li></ul></li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nilmtk import DataSet\n",
    "from nilmtk.utils import print_dict\n",
    "\n",
    "# Seeing dataset metadata\n",
    "refit = DataSet(out_dir)\n",
    "print_dict(refit.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1055cca6-a09d-4c49-8a2a-8d0737c575b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ul><li><strong>1</strong>: Building(instance=1, dataset='REFIT')</li><li><strong>10</strong>: Building(instance=10, dataset='REFIT')</li><li><strong>11</strong>: Building(instance=11, dataset='REFIT')</li><li><strong>12</strong>: Building(instance=12, dataset='REFIT')</li><li><strong>13</strong>: Building(instance=13, dataset='REFIT')</li><li><strong>14</strong>: Building(instance=14, dataset='REFIT')</li><li><strong>15</strong>: Building(instance=15, dataset='REFIT')</li><li><strong>16</strong>: Building(instance=16, dataset='REFIT')</li><li><strong>17</strong>: Building(instance=17, dataset='REFIT')</li><li><strong>18</strong>: Building(instance=18, dataset='REFIT')</li><li><strong>19</strong>: Building(instance=19, dataset='REFIT')</li><li><strong>2</strong>: Building(instance=2, dataset='REFIT')</li><li><strong>20</strong>: Building(instance=20, dataset='REFIT')</li><li><strong>3</strong>: Building(instance=3, dataset='REFIT')</li><li><strong>4</strong>: Building(instance=4, dataset='REFIT')</li><li><strong>5</strong>: Building(instance=5, dataset='REFIT')</li><li><strong>6</strong>: Building(instance=6, dataset='REFIT')</li><li><strong>7</strong>: Building(instance=7, dataset='REFIT')</li><li><strong>8</strong>: Building(instance=8, dataset='REFIT')</li><li><strong>9</strong>: Building(instance=9, dataset='REFIT')</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_dict(refit.buildings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5ae4f8a-9953-4784-8e10-eab61ea1de56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum start date across all buildings: 2013-09-17 23:00:00+00:00\n",
      "Maximum end date across all buildings: 2015-07-10 11:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dictionary to hold the date ranges for each house\n",
    "date_ranges = {}\n",
    "\n",
    "for house_id in refit.buildings:\n",
    "    house = refit.buildings[house_id]\n",
    "    \n",
    "    # The entire house\n",
    "    mains = house.elec.mains()\n",
    "    \n",
    "    # Get the first and last sample timestamps\n",
    "    start = pd.to_datetime(mains.get_timeframe().start).tz_convert('UTC')\n",
    "    end = pd.to_datetime(mains.get_timeframe().end).tz_convert('UTC')\n",
    "    start = start + pd.Timedelta(hours=1) - pd.Timedelta(minutes=start.minute, seconds=start.second, microseconds=start.microsecond)\n",
    "    end = end - pd.Timedelta(minutes=end.minute, seconds=end.second, microseconds=end.microsecond)\n",
    "    \n",
    "    # Store the date range for this building\n",
    "    date_ranges[house_id] = (start, end)\n",
    "\n",
    "min_date = min(date_ranges.values(), key=lambda x: x[0])[0]\n",
    "max_date = max(date_ranges.values(), key=lambda x: x[1])[1]\n",
    "\n",
    "# Output the results\n",
    "print(\"Minimum start date across all buildings:\", min_date)\n",
    "print(\"Maximum end date across all buildings:\", max_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bb29f21-3e3f-4947-b03b-8144c83f4486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fridge\n",
      "freezer\n",
      "freezer\n",
      "washer dryer\n",
      "washing machine\n",
      "dish washer\n",
      "computer\n",
      "television\n",
      "electric space heater\n"
     ]
    }
   ],
   "source": [
    "for meter in refit.buildings[1].elec.submeters().meters:\n",
    "    for appliance in meter.appliances:\n",
    "        print(appliance.metadata.get('type'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b051df3-c7f0-4474-9ffa-56a8a2c8de55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fridge: [(1, 1), (11, 1), (17, 1), (19, 1), (4, 1), (7, 1), (8, 1)]\n",
      "freezer: [(1, 2), (1, 3), (10, 3), (13, 2), (16, 1), (17, 2), (19, 2), (3, 3), (4, 2), (6, 1), (7, 2), (7, 3), (8, 2)]\n",
      "washer dryer: [(1, 4), (17, 4), (8, 3), (9, 2)]\n",
      "washing machine: [(1, 5), (10, 5), (11, 3), (13, 3), (14, 3), (15, 5), (16, 4), (17, 5), (18, 2), (19, 4), (2, 2), (20, 3), (3, 6), (4, 5), (4, 6), (5, 3), (6, 2), (7, 5), (8, 4), (9, 3)]\n",
      "dish washer: [(1, 6), (10, 6), (11, 4), (13, 4), (14, 4), (15, 6), (17, 6), (19, 5), (2, 3), (20, 4), (3, 5), (5, 4), (6, 3), (7, 6), (9, 4)]\n",
      "computer: [(1, 7), (11, 5), (12, 4), (14, 5), (15, 7), (16, 5), (17, 7), (19, 6), (5, 5), (6, 4), (6, 9), (8, 6)]\n",
      "television: [(1, 8), (10, 7), (12, 8), (13, 1), (14, 6), (15, 8), (16, 6), (16, 9), (17, 8), (18, 3), (19, 7), (2, 4), (20, 6), (3, 7), (4, 7), (5, 6), (6, 5), (7, 7), (8, 7), (9, 5)]\n",
      "electric space heater: [(1, 9), (15, 3), (15, 4), (9, 9)]\n",
      "food processor: [(10, 1), (10, 9), (20, 5)]\n",
      "toaster: [(10, 2), (12, 7), (14, 9), (18, 6), (2, 6), (3, 1), (5, 9), (6, 8), (7, 8), (8, 5)]\n",
      "fridge freezer: [(10, 4), (11, 2), (12, 1), (14, 1), (15, 1), (15, 2), (16, 2), (17, 3), (18, 1), (2, 1), (20, 1), (3, 2), (4, 3), (5, 1), (9, 1)]\n",
      "microwave: [(10, 8), (11, 6), (12, 5), (13, 7), (13, 8), (14, 7), (16, 7), (17, 9), (18, 4), (19, 8), (2, 5), (3, 8), (4, 8), (5, 7), (6, 6), (8, 8), (9, 6)]\n",
      "kettle: [(11, 7), (12, 6), (13, 9), (16, 8), (18, 5), (19, 9), (2, 8), (3, 9), (4, 9), (5, 8), (6, 7), (7, 9), (8, 9), (9, 7)]\n",
      "broadband router: [(11, 8), (13, 6)]\n",
      "audio system: [(11, 9), (14, 8), (18, 9), (2, 7), (9, 8)]\n",
      "unknown: [(12, 2), (12, 3), (12, 9), (13, 5), (20, 7)]\n",
      "tumble dryer: [(14, 2), (16, 3), (19, 3), (20, 2), (3, 4), (4, 4), (5, 2), (7, 4)]\n",
      "dehumidifier: [(15, 9)]\n",
      "breadmaker: [(18, 7)]\n",
      "games console: [(18, 8)]\n",
      "fan: [(2, 9)]\n",
      "appliance: [(20, 8)]\n",
      "pond pump: [(20, 9)]\n"
     ]
    }
   ],
   "source": [
    "# device types\n",
    "device_types = {}\n",
    "\n",
    "for house_id in refit.buildings:\n",
    "    elec = refit.buildings[house_id].elec\n",
    "    for num, meter in enumerate(elec.submeters().meters, start=1):\n",
    "        #help(meter.load)\n",
    "        for appliance in meter.appliances:\n",
    "            if appliance.metadata.get('type') is not None:\n",
    "                appliance_type = appliance.metadata.get('type')\n",
    "                # Sorting them into types with (house, applianceNum) tuples\n",
    "                if appliance_type not in device_types:\n",
    "                    device_types[appliance_type] = [(meter.building(), num)]\n",
    "                else:\n",
    "                    device_types[appliance_type].append((meter.building(), num))\n",
    "            else:\n",
    "                print('Nothing there')\n",
    "\n",
    "# Print out the device types and their associated meter instances\n",
    "for appliance_type, house in device_types.items():\n",
    "    print(f\"{appliance_type}: {house}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3dd7533-3876-4b6b-9257-5ad0b17a9678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlwaysOn: {1: {1, 2, 3}, 11: {8, 1, 2}, 17: {1, 2, 3}, 19: {1, 2}, 4: {1, 2, 3}, 7: {1, 2, 3}, 8: {1, 2}, 10: {3, 4}, 13: {2, 6}, 16: {1, 2}, 3: {2, 3}, 6: {1}, 12: {1}, 14: {1}, 15: {1, 2}, 18: {1}, 2: {1}, 20: {1, 9}, 5: {1}, 9: {1}}\n",
      "Intermit: {1: {4, 5, 6, 7, 8, 9}, 17: {4, 5, 6, 7, 8, 9}, 8: {3, 4, 5, 6, 7, 8, 9}, 9: {2, 3, 4, 5, 6, 7, 8, 9}, 10: {1, 2, 5, 6, 7, 8, 9}, 11: {3, 4, 5, 6, 7, 9}, 13: {1, 3, 4, 5, 7, 8, 9}, 14: {2, 3, 4, 5, 6, 7, 8, 9}, 15: {3, 4, 5, 6, 7, 8, 9}, 16: {3, 4, 5, 6, 7, 8, 9}, 18: {2, 3, 4, 5, 6, 7, 8, 9}, 19: {3, 4, 5, 6, 7, 8, 9}, 2: {2, 3, 4, 5, 6, 7, 8, 9}, 20: {2, 3, 4, 5, 6, 7, 8}, 3: {1, 4, 5, 6, 7, 8, 9}, 4: {4, 5, 6, 7, 8, 9}, 5: {2, 3, 4, 5, 6, 7, 8, 9}, 6: {2, 3, 4, 5, 6, 7, 8, 9}, 7: {4, 5, 6, 7, 8, 9}, 12: {2, 3, 4, 5, 6, 7, 8, 9}}\n"
     ]
    }
   ],
   "source": [
    "# Putting the devices into groups. Possibly separate by water consumption after.\n",
    "# Not in this set then it is intermittent\n",
    "# Add more if you notice more above in different datasets\n",
    "always_on_types = set()\n",
    "always_on_types.update(['fridge', 'freezer', 'fridge freezer', 'broadband router', 'pond pump'])\n",
    "condensed_types = {'AlwaysOn': {}, 'Intermit': {}}\n",
    "\n",
    "# Sorting all of the appliances into distinct categories\n",
    "for appliance_type in device_types.keys():\n",
    "    if appliance_type in always_on_types:\n",
    "        for (house, num) in device_types[appliance_type]:\n",
    "            if house not in condensed_types['AlwaysOn']:\n",
    "                condensed_types['AlwaysOn'][house] = set()\n",
    "                \n",
    "            condensed_types['AlwaysOn'][house].add(num)\n",
    "    else:\n",
    "        for (house, num) in device_types[appliance_type]:\n",
    "            if house not in condensed_types['Intermit']:\n",
    "                condensed_types['Intermit'][house] = set()\n",
    "                \n",
    "            condensed_types['Intermit'][house].add(num)\n",
    "\n",
    "# Print out the separated categories\n",
    "for cat, info in condensed_types.items():\n",
    "    print(f\"{cat}: {info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5059f5d-c7a9-4a8e-a959-69f5590b45f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of row processing\n",
    "def process_row(row):\n",
    "    return row.nlargest(k).iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74aa1cbb-aee6-4d56-8465-d684b4030e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total invalid rows: 23850\n",
      "Found incorrect mains sum for house 1 (iter 1)\n",
      "Total invalid rows: 224\n",
      "Found incorrect mains sum for house 1 (iter 2)\n",
      "Total invalid rows: 14\n",
      "Found incorrect mains sum for house 1 (iter 3)\n",
      "Total invalid rows: 3\n",
      "Found incorrect mains sum for house 1 (iter 4)\n",
      "Total invalid rows: 0\n",
      "Done checking all of house 1\n",
      "Total invalid rows: 28444\n",
      "Found incorrect mains sum for house 2 (iter 1)\n",
      "Total invalid rows: 124\n",
      "Found incorrect mains sum for house 2 (iter 2)\n",
      "Total invalid rows: 9\n",
      "Found incorrect mains sum for house 2 (iter 3)\n",
      "Total invalid rows: 0\n",
      "Done checking all of house 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num, meter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(elec\u001b[38;5;241m.\u001b[39msubmeters()\u001b[38;5;241m.\u001b[39mmeters, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Checking if the appliance has any larger values\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     meter_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28mnext\u001b[39m(meter\u001b[38;5;241m.\u001b[39mload()))\n\u001b[1;32m---> 14\u001b[0m     all_meters_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mall_meters_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeter_df\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Calculate the sum of all meters for each row\u001b[39;00m\n\u001b[0;32m     17\u001b[0m sum_of_meters \u001b[38;5;241m=\u001b[39m all_meters_df\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Ecolux\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:258\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03mConcatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03malong the other axes.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03mValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    245\u001b[0m     objs,\n\u001b[0;32m    246\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    255\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    256\u001b[0m )\n\u001b[1;32m--> 258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Ecolux\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:472\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    468\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mreindex(new_labels)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    470\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_data, indexers))\n\u001b[1;32m--> 472\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_block_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy:\n\u001b[0;32m    476\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Ecolux\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2044\u001b[0m, in \u001b[0;36mconcatenate_block_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m   2042\u001b[0m values \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m-> 2044\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2045\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m copy:\n\u001b[0;32m   2046\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mview()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corrected_mains = {}\n",
    "\n",
    "# I want to prevent mismeasurements by adding the W of any column\n",
    "for house_id in sorted(refit.buildings):\n",
    "    elec = refit.buildings[house_id].elec\n",
    "    mains = elec.mains()\n",
    "    mains_df = pd.DataFrame(next(mains.load()))\n",
    "    all_meters_df = pd.DataFrame()\n",
    "\n",
    "    # Going through appliances\n",
    "    for num, meter in enumerate(elec.submeters().meters, start=1):\n",
    "        # Checking if the appliance has any larger values\n",
    "        meter_df = pd.DataFrame(next(meter.load()))\n",
    "        all_meters_df = pd.concat([all_meters_df, meter_df], axis=1)\n",
    "\n",
    "    # Calculate the sum of all meters for each row\n",
    "    sum_of_meters = all_meters_df.sum(axis=1)\n",
    "\n",
    "    # Rare case where sum is still less so get the highest val iteratively\n",
    "    for k in range(1, len(elec.submeters().meters) + 1):\n",
    "        condition = sum_of_meters > mains_df.squeeze()  # Assuming mains_df is a single column DataFrame\n",
    "        print(\"Total invalid rows: \" + str(condition.sum()))\n",
    "\n",
    "        # If the condition is empty we don't have to check the next highest meter\n",
    "        if not condition.any():\n",
    "            break\n",
    "\n",
    "        # Add the highest val to mains\n",
    "        print(\"Found incorrect mains sum for house \" + str(house_id) + \" (iter \" + str(k) + \")\")\n",
    "        \n",
    "        # This is to take advantage of the added speed of max\n",
    "        if k == 1:\n",
    "            mains_df[condition] += all_meters_df[condition].max(axis=1).values.reshape(-1, 1)\n",
    "        else:\n",
    "            mains_df[condition] += all_meters_df[condition].apply(process_row, axis=1).values.reshape(-1, 1)\n",
    "\n",
    "    # Add to corrected\n",
    "    corrected_mains[house_id] = mains_df\n",
    "    print('Done checking all of house ' + str(house_id))\n",
    "\n",
    "print('Finished checking all houses')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4316073d-6de2-4a80-8274-f4a213f8a0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the time to int with error handling. For time values\n",
    "def convert_time_to_seconds(time_obj):\n",
    "    return time_obj.hour * 3600 + time_obj.minute * 60 + time_obj.second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b2dec8-4b5b-42bf-8f9d-db5a686a1a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Directory where you want to save the CSV files\n",
    "csv_dir = 'C:\\\\Users\\\\nicho\\\\OneDrive - The University of Western Ontario\\\\Ecolux\\\\Databases\\\\REFIT\\\\Hourly CSV\\\\'\n",
    "if not os.path.exists(csv_dir):\n",
    "    os.makedirs(csv_dir)\n",
    "\n",
    "for house_id, (start, end) in sorted_date_ranges:\n",
    "    # Initialize an empty DataFrame for the building\n",
    "    house_df = pd.DataFrame()\n",
    "\n",
    "    # Access electricity data for the house\n",
    "    elec = refit.buildings[house_id].elec\n",
    "    mains_df = pd.DataFrame()\n",
    "\n",
    "    # Loading the mains data and resampling\n",
    "    thirty_sec_mains = corrected_mains[house_id].resample('30S').mean()\n",
    "    thirty_sec_energy = thirty_sec_mains * (1/(120)) # Converting to Wh\n",
    "    mains_df = thirty_sec_energy.resample('H').sum().round(3)\n",
    "    \n",
    "    # Two data frames for each category\n",
    "    cat_df = {'AlwaysOn': pd.DataFrame(), 'Intermit': pd.DataFrame()}\n",
    "\n",
    "    # Go through each meter\n",
    "    for num, meter in enumerate(elec.submeters().meters, start=1):\n",
    "        print(f\"Processing appiance {num} for house {house_id}\")\n",
    "\n",
    "        # Finding the category\n",
    "        for cat, house in condensed_types.items():\n",
    "            if house_id in house and num in house[house_id]:\n",
    "                meter_cat = cat\n",
    "        try:\n",
    "            # Load the meter data with hour sampling\n",
    "            # TODO CHANGE TO HANDLE MULTIPLE CHUNKS\n",
    "            meter_data = next(meter.load()) \n",
    "            \n",
    "            if not meter_data.empty:\n",
    "                # Resample to hourly data and sum the readings\n",
    "                thirty_sec_app_power = meter_data.resample('30S').mean()\n",
    "                thirty_sec_app_energy = thirty_sec_app_power * (1/(120)) # Converting to Wh\n",
    "                hourly_data = thirty_sec_app_energy.resample('H').sum().round(3)\n",
    "\n",
    "            # Putting the meter in a category\n",
    "            if cat_df[meter_cat].empty:\n",
    "                cat_df[meter_cat] = hourly_data\n",
    "            else:\n",
    "                cat_df[meter_cat] += hourly_data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing Meter{num} for House {house_id}. Error: {e}\")\n",
    "\n",
    "    # Combine the category DataFrames and handle missing data\n",
    "    for cat, df in cat_df.items():\n",
    "        # GOTTA HANDLE THE ZERO VALUES CORRECTLY\n",
    "        df.fillna(method='ffill', inplace=True)  # Replace NaNs with a forward fill\n",
    "        house_df[cat] = df.sum(axis=1)  # Sum across meters within the category\n",
    "\n",
    "    # Adding the mains data\n",
    "    house_df['Total'] = mains_df\n",
    "    house_df['HVAC'] = house_df['Total'] - house_df['AlwaysOn'] - house_df['Intermit']\n",
    "    \n",
    "    # Converting time zones only if needed\n",
    "    #house_df.index = house_df.index.tz_convert('Europe/London')\n",
    "    house_df['DayNum'] = house_df.index.dayofweek\n",
    "    house_df['Time'] = house_df.index.time\n",
    "    house_df['Time'] = house_df['Time'].apply(convert_time_to_seconds)\n",
    "    house_df['Month'] = house_df.index.month\n",
    "\n",
    "    # Constants for cycles\n",
    "    seconds_in_a_day = 24 * 60 * 60\n",
    "    days_in_a_week = 7\n",
    "    months_in_a_year = 12\n",
    "    \n",
    "    # Adding sine and cos for all time varying values\n",
    "    house_df['TimeSin'] = np.sin(2 * np.pi * house_df['Time'] / seconds_in_a_day)\n",
    "    house_df['TimeCos'] = np.cos(2 * np.pi * house_df['Time'] / seconds_in_a_day)\n",
    "    house_df['DayNumSin'] = np.sin(2 * np.pi * house_df['DayNum'] / days_in_a_week)\n",
    "    house_df['DayNumCos'] = np.cos(2 * np.pi * house_df['DayNum'] / days_in_a_week)\n",
    "    house_df['MonthSin'] = np.sin(2 * np.pi * house_df['Month'] / months_in_a_year)\n",
    "    house_df['MonthCos'] = np.cos(2 * np.pi * house_df['Month'] / months_in_a_year)\n",
    "\n",
    "    # Only taking the columns I want\n",
    "    cols = ['DayNumSin', 'DayNumCos', 'TimeSin', 'TimeCos', 'MonthSin', 'MonthCos', 'Total', 'AlwaysOn', 'Intermit', 'HVAC']\n",
    "    house_df = house_df[cols]\n",
    "\n",
    "    # Write the combined building data to CSV\n",
    "    csv_file_path = os.path.join(csv_dir, f\"house{house_id}_clean.csv\")\n",
    "    house_df.to_csv(csv_file_path, index=True)\n",
    "    print(f\"Combined hourly data for house {house_id} written to CSV.\")\n",
    "\n",
    "print(\"Finished processing all buildings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61a9100-84f2-4d6e-bb89-841071d9b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_path = 'C:\\\\Users\\\\nicho\\\\OneDrive - The University of Western Ontario\\\\Ecolux\\\\Databases\\\\REFIT\\\\Hourly CSV\\\\house'\n",
    "weather_path = 'C:\\\\Users\\\\nicho\\\\OneDrive - The University of Western Ontario\\\\Ecolux\\\\Databases\\\\REFIT\\\\Weather CSV\\\\REFITWeather.csv'\n",
    "output_path = 'C:\\\\Users\\\\nicho\\\\OneDrive - The University of Western Ontario\\\\Ecolux\\\\Databases\\\\REFIT\\\\Completed CSV\\\\house'\n",
    "\n",
    "for house_id in sorted(refit.buildings):\n",
    "    cur_house = houses_path + str(house_id) + '_clean.csv'\n",
    "    cur_output = output_path + str(house_id) + '.csv'\n",
    "\n",
    "    # Reading the csvs\n",
    "    house_df = pd.read_csv(cur_house, index_col='Unix', parse_dates=['Unix'])\n",
    "    weather_df = pd.read_csv(weather_path, skiprows=3)\n",
    "    \n",
    "    # Convert to datetime\n",
    "    weather_df['time'] = pd.to_datetime(weather_df['time'], unit='s').dt.tz_localize('UTC').dt.tz_convert('Europe/London')\n",
    "    \n",
    "    # Set datetime as the index\n",
    "    weather_df.set_index('time', inplace=True)\n",
    "    \n",
    "    # Merge the dataframes based on the index\n",
    "    merged_df = house_df.join(weather_df, how='inner')\n",
    "\n",
    "    # Label the index\n",
    "    merged_df.reset_index(inplace=True)\n",
    "    merged_df.rename(columns={'index': 'Timestamp'}, inplace=True)\n",
    "\n",
    "    # Changing the weather names\n",
    "    merged_df['RealTemp'] = merged_df['temperature_2m (°C)']\n",
    "    merged_df['ApparTemp'] = merged_df['apparent_temperature (°C)']\n",
    "    merged_df['Humid'] = merged_df['relative_humidity_2m (%)']\n",
    "    merged_df['WmoCode'] = merged_df['weather_code (wmo code)']\n",
    "\n",
    "    # Organizing the columns\n",
    "    cols = ['TimeSin', 'TimeCos', 'DayNumSin', 'DayNumCos', 'MonthSin', 'MonthCos', 'RealTemp', 'ApparTemp', 'Humid', 'WmoCode', 'Total', 'AlwaysOn', 'Intermit', 'HVAC']\n",
    "    merged_df = merged_df[cols]\n",
    "\n",
    "    # Send to csv\n",
    "    merged_df.to_csv(cur_output, index=False)\n",
    "    print('Done formatting house ' + str(house_id))\n",
    "\n",
    "print('Done formatting all houses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c417381-d9d2-4c9a-9acf-7107b451c3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added meta data for house 1\n",
      "Added meta data for house 2\n",
      "Added meta data for house 3\n",
      "Added meta data for house 4\n",
      "Added meta data for house 5\n",
      "Added meta data for house 6\n",
      "Added meta data for house 7\n",
      "Added meta data for house 8\n",
      "Added meta data for house 9\n",
      "Added meta data for house 10\n",
      "Added meta data for house 11\n",
      "Added meta data for house 12\n",
      "Added meta data for house 13\n",
      "Added meta data for house 14\n",
      "Added meta data for house 15\n",
      "Added meta data for house 16\n",
      "Added meta data for house 17\n",
      "Added meta data for house 18\n",
      "Added meta data for house 19\n",
      "Added meta data for house 20\n"
     ]
    }
   ],
   "source": [
    "# Get metadata\n",
    "meta_input_path = 'C:\\\\Users\\\\nicho\\\\OneDrive - The University of Western Ontario\\\\Ecolux\\\\Databases\\\\REFIT\\\\MetaData\\\\HouseMetaData.csv'\n",
    "meta_df = pd.read_csv(meta_input_path)\n",
    "\n",
    "# For the houses\n",
    "all_houses_path = 'C:\\\\Users\\\\nicho\\\\OneDrive - The University of Western Ontario\\\\Ecolux\\\\Databases\\\\REFIT\\\\Completed CSV\\\\house'\n",
    "output_path = 'C:\\\\Users\\\\nicho\\\\OneDrive - The University of Western Ontario\\\\Ecolux\\\\Databases\\\\REFIT\\\\Completed With Meta\\\\house'\n",
    "\n",
    "# Loop through all the houses\n",
    "for house_id in range(1, len(refit.buildings) + 1):\n",
    "    # Meta data. Insuring it only takes one row\n",
    "    house_md = meta_df[meta_df['House'] == house_id]\n",
    "    house_md = house_md.iloc[0:1]\n",
    "    house_md = house_md.drop(['House'], axis=1)\n",
    "\n",
    "    # House data\n",
    "    house_path = all_houses_path + str(house_id) + '.csv'\n",
    "    house_data = pd.read_csv(house_path)\n",
    "\n",
    "    # Split in two leaving the usage values\n",
    "    split_position = len(house_data.columns) - 4\n",
    "    first_part = house_data.iloc[:, :split_position]\n",
    "    second_part = house_data.iloc[:, split_position:]\n",
    "\n",
    "    # Copying this row accross the entire data length and concat\n",
    "    repeated_md = pd.concat([house_md]*len(house_data), ignore_index=True)\n",
    "    result_df = pd.concat([first_part, repeated_md, second_part], axis=1)\n",
    "\n",
    "    house_output_path = output_path + str(house_id) + '.csv'\n",
    "    result_df.to_csv(house_output_path, index=False)\n",
    "\n",
    "    print('Added meta data for house ' + str(house_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be36be29-040e-4624-b4dc-6da8c846ac7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted house 1\n",
      "Formatted house 2\n",
      "Formatted house 3\n",
      "Formatted house 4\n",
      "Formatted house 5\n",
      "Formatted house 6\n",
      "Formatted house 7\n",
      "Formatted house 8\n",
      "Formatted house 9\n",
      "Formatted house 10\n",
      "Formatted house 11\n",
      "Formatted house 12\n",
      "Formatted house 13\n",
      "Formatted house 14\n",
      "Formatted house 15\n",
      "Formatted house 16\n",
      "Formatted house 17\n",
      "Formatted house 18\n",
      "Formatted house 19\n",
      "Formatted house 20\n"
     ]
    }
   ],
   "source": [
    "# Rem\n",
    "input_path = 'C:\\\\Users\\\\nicho\\\\OneDrive - The University of Western Ontario\\\\Ecolux\\\\Databases\\\\REFIT\\\\Completed With Meta\\\\house'\n",
    "output_path = 'C:\\\\Users\\\\nicho\\\\OneDrive - The University of Western Ontario\\\\Ecolux\\\\Databases\\\\REFIT\\\\Regression Training Set\\\\house'\n",
    "\n",
    "# Go through all houses\n",
    "for house_id in sorted(refit.buildings):\n",
    "    cur_input = input_path + str(house_id) + '.csv'\n",
    "    cur_output = output_path + str(house_id) + '.csv'\n",
    "\n",
    "    # Read csv\n",
    "    house_df = pd.read_csv(cur_input)\n",
    "\n",
    "    # Make conditional series\n",
    "    condition = house_df['Total'].squeeze() != 0\n",
    "\n",
    "    # Getting rid of extra rows\n",
    "    house_df = house_df[condition]\n",
    "    house_df.to_csv(cur_output, index=False)\n",
    "    print(\"Formatted house \" + str(house_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1711277c-94e3-4527-a17c-a21a1e130d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
